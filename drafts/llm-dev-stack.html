<!DOCTYPE html>
<html lang="en">
<head>
        <title>An LLM dev stack</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
          <link rel="stylesheet" href="https://ianbicking.org/theme/css/style.min.css">
        <!--<link rel="stylesheet" href="https://ianbicking.org/theme/css/main.css" type="text/css" />-->
        <link href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="Ian Bicking: a blog Atom Feed" />
        <link rel="icon" href="https://ianbicking.org/favicon.ico">

</head>

<body id="index" class="home">
  <div id="main-wrapper1">
  <div id="main-wrapper2">
  <div id="main-container">
        <header id="banner" class="body">
                <h1><a href="https://ianbicking.org">Ian Bicking: a blog </a></h1>
        </header><!-- /#banner -->
<section id="content" class="body">
    <article>
        <header>
        <div>
            <div class="published" title="2023-12-25T00:00:00-06:00">
                Monday, December 25th, 2023
            </div>
            <h1 class="entry-title">
                <a
                    href="https://ianbicking.org/drafts/llm-dev-stack.html"
                    rel="bookmark"
                    title="Permalink to An LLM dev stack"
                    >An <span class="caps">LLM</span> dev&nbsp;stack</a
                >
            </h1>
        </div>
        </header>

        <div class="entry-content"><p>I&#8217;ve been working with <span class="caps">LLM</span>/<span class="caps">GPT</span>-based processes and interfaces for a bit over a year and a half now&#8230; first independently and now also professionally at <a href="https://brilliant.org/">Brilliant.org</a>. Along the way I&#8217;ve built up a stack to do that work<sup id="fnref:0"><a class="footnote-ref" href="#fn:0">1</a></sup>. It&#8217;s very particular, with positive and negative attributes, but I&#8217;ve personally found it very pleasant to work&nbsp;in.</p>
<p>This work is still very trapped in my own work process and not open, but I thought it would be useful to at least share the shape of&nbsp;it.</p>
<p>If you&#8217;ve watched <a href="https://www.youtube.com/playlist?list=PLzoCgYJlxwLtELaZdMHkgiHvwhuqjtHZ0">any of my videos</a> you&#8217;ll have seen different iterations of what I describe&nbsp;below.</p>
<h2>From a high&nbsp;level&#8230;</h2>
<p>The tools I make are transparent and generally represent some kind of experiment. The &#8220;user&#8221; is also a trusted collaborator, a fellow employee, or just another <span class="caps">LLM</span> enthusiast. These tools then both expose the inner workings, but also try to present the results in a meaningful way (for instance with rendered&nbsp;LaTeX).</p>
<p>Since my time at Mozilla I&#8217;ve shifted to the frontend, using minimal and generalized backend services. These tools for working with LLMs also are almost entirely in the browser: taking user input, fetching additional data, sending requests to <span class="caps">LLM</span> services, orchestrating and visualizing the results. I like having a live view of the running system, including all the intermediate steps. I never abstract away or hide the actual prompts and requests (an enthusiastic yes to: <a href="https://hamel.dev/blog/posts/prompt/">Fuck You, Show Me The Prompt.</a>).</p>
<p>I have <em>not</em> built a framework. There are common components and I&#8217;ve used those to make chat interfaces, data analysis, math feedback and critique, sometimes with cameras and voice, and with human-in-the-loop at many steps. The web is very flexible and it&#8217;s possible to experiment with lots of different ways to package functionality in a <span class="caps">UI</span>.</p>
<p>Instead of a framework I have a number of complementary&nbsp;libraries:</p>
<ol>
<li><a href="#llm-api">Basic access to LLMs</a> (<code>llm.chat({messages: [...]}))</code>)</li>
<li><a href="#llm-logging">A logging component</a> for <span class="caps">LLM</span> requests that displays request/response in the&nbsp;browser</li>
<li><a href="#templating">A templating&nbsp;language</a></li>
<li><a href="#prompt-templates">An abstraction over <code>llm.chat()</code> to create &#8220;app prompts&#8221;</a>. This is the biggest and most framework-like piece, with considerable effort to <a href="#typing">add types to templates and&nbsp;tools</a></li>
<li><a href="#signals-persistence">A simple persistence layer built on signals</a>, both to serialize state and update the <span class="caps">UI</span><ol>
<li><a href="#rpc-database">An an <span class="caps">RPC</span> system for handling database&nbsp;interactions</a></li>
</ol>
</li>
<li><a href="#component-library">Higher-level React components</a> for form input, Markdown, tabs, and other <span class="caps">UI</span> I frequently make use of. These integrate closely with&nbsp;signals.</li>
<li>A very much in-progress batch processing&nbsp;system</li>
<li><a href="#not-implemented">Topics where I neither write nor use a library&#8230;</a><ol>
<li><a href="#pipelines">Pipelines</a></li>
<li><a href="#rag"><span class="caps">RAG</span></a></li>
<li><a href="#evaluation">Evaluation</a></li>
</ol>
</li>
</ol>
<p>Lastly I discuss <a href="#what-doesnt-work-well">what doesn&#8217;t work&nbsp;well</a></p>
<h3>Dependencies</h3>
<p>I try to keep my dependencies under control and use larger conventional&nbsp;libraries:</p>
<ol>
<li>React</li>
<li>Tailwind <span class="caps">CSS</span></li>
<li>Next.js on&nbsp;Vercel</li>
<li><a href="https://preactjs.com/guide/v10/signals/">Preact&nbsp;Signals</a></li>
<li>Some simple server functions and PostgreSQL on the&nbsp;backend</li>
</ol>
<p>And some miscellany: <a href="https://github.com/kpdecker/jsdiff">diff</a>, <a href="https://highlightjs.org/">highlight.js</a>, <a href="https://www.npmjs.com/package/@cfworker/json-schema">json-schema</a>, some <a href="https://github.com/angus-c/just">just</a> libraries, <a href="https://katex.org/">katex</a>, <a href="https://github.com/markedjs/marked">marked</a>, <a href="https://ohmjs.org/">ohm</a></p>
<h2><span id="llm-api"><span class="caps">LLM</span> <span class="caps">API</span></span></h2>
<p>The <span class="caps">API</span> I use for all the <span class="caps">LLM</span> calls looks like the underlying <span class="caps">GPT</span>&nbsp;call:</p>
<div class="highlight"><pre><span></span><span class="kr">const</span> <span class="nx">resp</span> <span class="o">=</span> <span class="nx">await</span> <span class="nx">llm</span><span class="p">.</span><span class="nx">chat</span><span class="p">({</span>
    <span class="nx">messages</span><span class="o">:</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nx">role</span><span class="o">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
            <span class="nx">content</span><span class="o">:</span> <span class="s2">&quot;You&#39;re a happy cat.&quot;</span><span class="p">,</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nx">role</span><span class="o">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span>
            <span class="nx">content</span><span class="o">:</span> <span class="s2">&quot;Hello!&quot;</span><span class="p">,</span>
        <span class="p">},</span>
    <span class="p">],</span>
<span class="p">});</span>
</pre></div>


<p>You can specify the model or temperature or just leave it as the default, with settings in the <span class="caps">UI</span> to override it. There&#8217;s also optional caching (which I only use&nbsp;occassionally).</p>
<p>Rate limiting and retries are handled automatically at this layer. Tool calls are also validated and the request retried if necessary.<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">2</a></sup></p>
<p>The <code>LlmResponse</code> object (<code>resp</code>) contains some helpful getters for content, functions/tool-calls, and parsing out code blocks.<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">3</a></sup></p>
<p>Because at work I use a lot of LaTeX for our math content, I also have special fixups for LaTeX escaping. <span class="caps">GPT</span> regularly mixes up how many backlashes to use when including LaTeX in <span class="caps">JSON</span>. It&#8217;s a reminder that <span class="caps">GPT</span> is really responding textually even when it generates <span class="caps">JSON</span>, it&#8217;s not generating native data&nbsp;structures.</p>
<p>I&#8217;ve never implemented streaming. It would be nice to be able to <em>see</em> things stream, but the complexity of <em>using</em> a stream seems overwhelming and error-prone. Seeing how other frameworks handle streaming reinforces this perception. I prefer&nbsp;patience.</p>
<p>For supporting different LLMs besides <span class="caps">GPT</span> I use <a href="https://openrouter.ai/">OpenRouter.ai</a>, though <a href="https://withmartian.com/">Martian</a> and <a href="https://www.braintrustdata.com/">Braintrust</a> are also good options. They all do <span class="caps">API</span> translation as a service as well as handle the billing across services. Generally Claude works well as a drop-in replacement for <span class="caps">GPT</span>. I am completely unable to get Gemini Pro 1.5 to handle tool calls properly, which <em>might</em> be due to the translation layer (but my past experiences using the Gemini <span class="caps">API</span> directly have given me low confidence in Gemini&#8217;s abilities&nbsp;here).</p>
<h2><span id="llm-logging">Logging <span class="caps">LLM</span>&nbsp;requests</span></h2>
<p><a href="../media/llm-dev-stack/screenshot-plain-log.png" target="_blank"><img style="float: right; width: 30%; border-radius: 5px; margin-left: 4px;" src="../media/llm-dev-stack/screenshot-plain-log.png" alt="An example of the LLM log" /></a></p>
<p>It&#8217;s very important to me to see each prompt exactly as it is sent. For this I have a component <code>&lt;llm.Log /&gt;</code>.<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">4</a></sup> <sup id="fnref:11"><a class="footnote-ref" href="#fn:11">5</a></sup></p>
<p>I can see how many requests there are running, see previous requests and responses, how much time the request is taking, and the exact tools in the request and response. I spend a <em>lot</em> of time looking at this&nbsp;log.</p>
<h2><span id="templating">A templating&nbsp;language</span></h2>
<p>This part feels a bit indulgent, but I like it. The very beginning of the language was adding a simple conditional to JavaScript <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals#tagged_templates">tagged string templates</a>:</p>
<div class="highlight"><pre><span></span><span class="nx">llm</span><span class="p">.</span><span class="nx">chat</span><span class="p">({</span>
    <span class="nx">role</span><span class="o">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span>
    <span class="nx">content</span>: <span class="kt">tmpl</span><span class="sb">`</span>
<span class="sb">    Invent 100 words for </span><span class="si">${</span><span class="nx">inspiration</span><span class="si">}</span><span class="sb"></span>

<span class="sb">    [[Make note of these user instructions: </span><span class="si">${</span><span class="nx">instructions</span><span class="si">}</span><span class="sb">]]</span>
<span class="sb">    `</span><span class="p">,</span>
<span class="p">});</span>
</pre></div>


<p><code>tmpl()</code> makes that act&nbsp;like:</p>
<div class="highlight"><pre><span></span><span class="sb">`Invent 100 words for </span><span class="si">${</span><span class="nx">toString</span><span class="p">(</span><span class="nx">inspiration</span><span class="p">)</span><span class="si">}</span><span class="sb"></span>

<span class="si">${</span>
    <span class="nx">instructions</span>
        <span class="o">?</span> <span class="sb">`Make note of these user instructions: </span><span class="si">${</span><span class="nx">toString</span><span class="p">(</span><span class="nx">instructions</span><span class="p">)</span><span class="si">}</span><span class="sb">`</span>
        <span class="o">:</span> <span class="s2">&quot;&quot;</span>
<span class="si">}</span><span class="sb"></span>
<span class="sb">`</span><span class="p">;</span>
</pre></div>


<p><a href="../media/llm-dev-stack/screenshot-html-log.png" target="_blank"><img style="float: right; width: 30%; border-radius: 5px; margin-left: 4px;" src="../media/llm-dev-stack/screenshot-html-log.png" alt="Screenshot example of the LLM log with HTML and colored markup" /></a></p>
<p><code>toString()</code> prints out objects as their <span class="caps">JSON</span> instead of <code>[Object object]</code>, changes null to <code>""</code>, and other handy things. Whitespace is always&nbsp;trimmed.</p>
<p>Since that beginning I&#8217;ve added a bunch of features and finally made a real parser (using <a href="https://ohmjs.org/">Ohm</a>), an <span class="caps">AST</span>, an interpreter for the language that produces an Abstract Execution Tree, and loops, conditionals, expressions, and more. It&#8217;s good at producing reasonably formatted English text, like you want to give to an <span class="caps">LLM</span>, and can be run as a tagged template or as a more normal template language with variables passed&nbsp;in.</p>
<p>I&#8217;ve started using all-caps for keywords like <code>IF {{var}} [[output]]</code> – the norm in templating languages is to use punctuation around any syntax (like <code>&lt;% ... %&gt;</code> or <code>{% ... %}</code>), and I neither like that nor think it&#8217;s&nbsp;necessary.</p>
<p>Because it executes to an Abstract Execution Tree you can take the result and turn it into <span class="caps">HTML</span> in addition to text. I have found this very helpful for understanding both the rendered template and the underlying&nbsp;structure.</p>
<p><a href="../media/llm-dev-stack/screenshot-of-template-error.png" target="_blank"><img style="float: right; width: 30%; border-radius: 5px; margin-left: 4px;" src="../media/llm-dev-stack/screenshot-of-template-error.png" alt="Screenshot of a template error message" /></a></p>
<p>Lastly I&#8217;ve begun to add some type checking so I can statically determine if the template uses unexpected variables<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">6</a></sup>. Ultimately I&#8217;d like to use this to combine the concepts of &#8220;template&#8221; and &#8220;function&#8221; using structured input and&nbsp;output.</p>
<h2><span id="prompt-templates">Prompt&nbsp;templates</span></h2>
<p>A &#8220;prompt&#8221; in my system is one task for the <span class="caps">LLM</span>. It has a name and example template, and anyone can make variant templates to try out different&nbsp;prompts.</p>
<p>At the code level a prompt often has a <a href="https://platform.openai.com/docs/guides/function-calling">&#8220;Tool&#8221; (or &#8220;function&#8221;)</a><sup id="fnref:13"><a class="footnote-ref" href="#fn:13">7</a></sup>. When using tools I almost always provide and require one tool response, effectively using it as a way to getting schema-driven output from the <span class="caps">LLM</span>.<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">8</a></sup></p>
<p>I treat users as aspiring prompt engineers. They can create a variant of any of the prompts by adding a new template. When running the pipeline they can choose that variant, or choose to use multiple variants for comparison.<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">9</a></sup></p>
<p>While I&#8217;ve tried to expose the prompts as much as I can, I have learned that it&#8217;s not easy to jump in and write these prompts. It&#8217;s not <em>technically</em> hard, but knowing what to do and what to try is hard, or intimidating. Still it is <a href="https://www.linkedin.com/posts/ianbicking_gpt-activity-7166539285007921152-aHo1">invaluable to have a domain expert drive these instructions</a>; I wish I knew better how to facilitate this codevelopment between engineer and domain&nbsp;expert.</p>
<p>One notable feature of variants is that in addition to modifying the prompt the variant can also change the parameter descriptions of the Tools. This is very important! The only right way to think about Tools is that the schema and descriptions are part of the prompt.<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">10</a></sup></p>
<p>Someday I&#8217;d like to make a kind of schema mini-language for the prompt templates so whole pipelines can be composed using&nbsp;these.</p>
<p>The templates look like this (for a prompt that uses a Tool with a <code>greetings</code> parameter):</p>
<div class="highlight"><pre><span></span>&gt;&gt;&gt; system
You&#39;re a happy cat

&gt;&gt;&gt; user
{{userInput}}

[[/* Example of setting a tool parameter description: */]]
&gt;&gt;&gt; greeting.description
Be silly
</pre></div>


<p>Some frameworks use <a href="https://js.langchain.com/v0.2/docs/concepts#chatprompttemplates">lists of templates</a>, one template for each message (a <code>system</code> and <code>user</code> message in this case). That is not very amenable to editing, storage, diffs, etc. If anything I want to keep jamming <em>more</em> options into these&nbsp;templates.</p>
<p>There&#8217;s not really room for an entire history in this style of template, but in my own work I seldom include a chat history (though I do separate &#8220;system&#8221; from &#8220;user&#8221;<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">12</a></sup>). When I do need to include history then that is done in code. Coming up with a canonical representation of the &#8220;middle&#8221; messages is usually find, as all the sophisticated prompting goes at the beginning and&nbsp;end.</p>
<h3><span id="typing">Typing</span></h3>
<p>After making a complicated pipeline with lots of prompts that fed into each other I kept getting bugs where I did not provide or store all the data I promised for the template, or the template referred to incorrect variables. Data integrity became quite difficult and it was one of several things left me exhausted and lacking confidence in any&nbsp;changes.</p>
<p>I then started rewriting <em>everything</em> in TypeScript. I&#8217;ve used TypeScript in the past but never happily. This time with ChatGPT and Copilot it was much more successful&#8230; small things that might trip me up no longer did when I could ask ChatGPT questions, and some of the more boring/repetitive/error-prone transformations were well handled by&nbsp;Copilot.</p>
<p>I also started adding types to the prompt input. Prompts now look like&nbsp;this:</p>
<div class="highlight"><pre><span></span><span class="kr">import</span> <span class="o">*</span> <span class="kr">as</span> <span class="nx">promptTypes</span> <span class="nx">from</span> <span class="s2">&quot;./prompts.types.ts&quot;</span><span class="p">;</span>

<span class="kr">interface</span> <span class="nx">HelloPromptInputType</span> <span class="p">{</span>
    <span class="nx">userInput</span>: <span class="kt">string</span><span class="p">;</span>
<span class="p">}</span>

<span class="kr">const</span> <span class="nx">helloPrompt</span> <span class="o">=</span> <span class="k">new</span> <span class="nx">AppPrompt</span><span class="o">&lt;</span>
    <span class="nx">HelloPromptInputsType</span><span class="p">,</span>
    <span class="nx">promptTypes</span><span class="p">.</span><span class="nx">HelloPromptResponseType</span>
<span class="o">&gt;</span><span class="p">({</span>
    <span class="nx">name</span><span class="o">:</span> <span class="s2">&quot;hello&quot;</span><span class="p">,</span>
    <span class="nx">description</span><span class="o">:</span> <span class="s2">&quot;Used when the first message comes in&quot;</span><span class="p">,</span>
    <span class="nx">inputSchema</span>: <span class="kt">promptTypes.helloPromptSchema</span><span class="p">,</span>
    <span class="nx">temperature</span>: <span class="kt">0.7</span><span class="p">,</span>
    <span class="nx">starterTemplate</span><span class="o">:</span> <span class="sb">`</span>
<span class="sb">    &gt;&gt;&gt; system</span>
<span class="sb">    You&#39;re a happy cat</span>
<span class="sb">    &gt;&gt;&gt; user</span>
<span class="sb">    {{userInput}}</span>
<span class="sb">    `</span><span class="p">,</span>
    <span class="nx">toolResponse</span><span class="o">:</span> <span class="p">{</span>
        <span class="nx">name</span><span class="o">:</span> <span class="s2">&quot;sayHello&quot;</span><span class="p">,</span>
        <span class="nx">parameters</span><span class="o">:</span> <span class="p">{</span>
            <span class="nx">type</span><span class="o">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
            <span class="nx">required</span><span class="o">:</span> <span class="p">[</span><span class="s2">&quot;greeting&quot;</span><span class="p">,</span> <span class="s2">&quot;message&quot;</span><span class="p">],</span>
            <span class="nx">properties</span><span class="o">:</span> <span class="p">{</span>
                <span class="nx">greeting</span><span class="o">:</span> <span class="p">{</span>
                    <span class="nx">type</span><span class="o">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
                    <span class="nx">description</span><span class="o">:</span> <span class="s2">&quot;The greeting to put in the header&quot;</span><span class="p">,</span>
                <span class="p">},</span>
                <span class="nx">message</span><span class="o">:</span> <span class="p">{</span>
                    <span class="nx">type</span><span class="o">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span>
                    <span class="nx">description</span><span class="o">:</span> <span class="s2">&quot;The friendly message for the user&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
        <span class="p">},</span>
    <span class="p">},</span>
<span class="p">});</span>
</pre></div>


<p>(I use <code>toolResponse</code> when I want the <span class="caps">LLM</span> to always respond with that one Tool, as opposed to offering a suite of tools that it can pick from. It also sets <a href="https://platform.openai.com/docs/guides/function-calling/function-calling-behavior">tool_choice</a>)</p>
<p>With this type information it will be a type error if I call the prompt without the proper input, and the return type will be typed according to the function&nbsp;response.</p>
<p>Things get a bit complicated as I bridge the world of TypeScript types and the <a href="https://json-schema.org/"><span class="caps">JSON</span> Schema</a> that is used to define Tools. I ended up deciding to use TypeScript types (like <code>HelloPromptInputType</code>) to define the input variables, and a <span class="caps">JSON</span> schema (<code>toolResponse</code>) to define the Tool. I use <em>both</em> <a href="https://github.com/YousefED/typescript-json-schema">typescript-json-schema</a> and <a href="https://github.com/bcherny/json-schema-to-typescript">json-schema-to-typescript</a> to generate the accompanying <code>promptTypes.ts</code> file that both translates the toolResponse schema into a type (<code>promptTypes.HelloPromptResponseType</code>) and <code>HelloPromptsInputType</code> into a schema (<code>promptTypes.helloPromptSchema</code>). This is a&nbsp;hassle.</p>
<p>The <code>inputSchema</code> is used as documentation and to check that templates don&#8217;t use invalid&nbsp;variables.</p>
<p>Upon reflection I think I&#8217;d like convert the Tool definitions (or at least their parameters) into types, and then move the prompt-related parts more fully into the template. In that model you&#8217;d add all your parameter descriptions via the template (even if technically they can be created with comments in the type&#8217;s source), and manage any other details that come up like parameter order. Then the types would be canonical, the <span class="caps">JSON</span> Schemas would be derived, and the type checker will be much&nbsp;happier.</p>
<h2><span id="signals-persistence">Signals <span class="amp">&amp;</span>&nbsp;Persistence</span></h2>
<p>This feels off-topic, as this is really just web frontend development and not related to LLMs. But to be productive I need a productive environment. And I <strong>really love signals</strong>, so I will go on this tangent as a&nbsp;fan.</p>
<p><a href="https://preactjs.com/guide/v10/signals/">Preact Signals</a> are boxes<sup id="fnref:15"><a class="footnote-ref" href="#fn:15">13</a></sup> with a <code>.value</code> attribute that you can get or set. It&#8217;s kind of like a global variable or a reference. The magic is that if you access <code>.value</code> in a React Component then if the value of the signal changes the component will be rerendered. You can also listen and react to changes from&nbsp;elsewhere.</p>
<p>Most of my persistence is one big <code>session</code> signal that is the data model for almost the entire page, and when updated it serializes the new session object to the database. I use lots of &#8220;views&#8221; like <code>signalView()</code> to create signals that reference just a part of the session tree, but appear as a stand-alone&nbsp;signal:</p>
<div class="highlight"><pre><span></span><span class="kr">const</span> <span class="nx">session</span> <span class="o">=</span> <span class="nx">makeSessionSignal</span><span class="p">();</span>
<span class="kr">const</span> <span class="nx">username</span> <span class="o">=</span> <span class="nx">signalView</span><span class="o">&lt;</span><span class="kt">string</span><span class="o">&gt;</span><span class="p">(</span><span class="nx">session</span><span class="p">,</span> <span class="s2">&quot;username&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">);</span>
<span class="c1">// Now this:</span>
<span class="nx">username</span><span class="p">.</span><span class="nx">value</span> <span class="o">=</span> <span class="s2">&quot;ian&quot;</span><span class="p">;</span>
<span class="c1">// Is equivalent to this:</span>
<span class="nx">session</span><span class="p">.</span><span class="nx">value</span> <span class="o">=</span> <span class="p">{</span>
    <span class="p">...</span><span class="nx">session</span><span class="p">.</span><span class="nx">value</span><span class="p">,</span>
    <span class="nx">username</span><span class="o">:</span> <span class="s2">&quot;ian&quot;</span><span class="p">,</span>
<span class="p">};</span>
<span class="c1">// And this is always true:</span>
<span class="nx">username</span><span class="p">.</span><span class="nx">value</span> <span class="o">===</span> <span class="nx">session</span><span class="p">.</span><span class="nx">value</span><span class="p">.</span><span class="nx">username</span><span class="p">;</span>
</pre></div>


<p>I have a similar view for arrays and many data structure helpers using&nbsp;signals.</p>
<h3><span id="rpc-database"><span class="caps">RPC</span> and database&nbsp;access</span></h3>
<p>I&#8217;ve always been a skeptical of the advantage of using the same language on the server and client. There&#8217;s different expectations, permission layers, different ways of thinking about scope, and so an explicit separation seemed useful. And Node.js was always just different enough from the browser to make it hard to really reuse&nbsp;code.</p>
<p>Those separations aren&#8217;t that important with my current work, and packaging systems and environments have been converging, so that it&#8217;s quite reasonable now to run the exact same code on the server and browser. I don&#8217;t literally do that very often, running the same code in both places, but I like the <em>appearance</em> of a singular codebase. And using TypeScript it&#8217;s common to want to use the same <em>types</em> on both&nbsp;sides.</p>
<p>Using this I&#8217;ve switched to <span class="caps">RPC</span> for all my database access, with no well-defined endpoints. Instead I wrap a function&nbsp;like:</p>
<div class="highlight"><pre><span></span><span class="kr">export</span> <span class="kr">const</span> <span class="nx">getLesson</span> <span class="o">=</span> <span class="nx">ServerQueries</span><span class="p">.</span><span class="nx">wrap</span><span class="p">(</span><span class="nx">async</span> <span class="p">(</span><span class="nx">lessonId</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="k">return</span> <span class="nx">dbExec</span><span class="p">(</span><span class="nx">sql</span><span class="sb">`</span>
<span class="sb">    SELECT * FROM lessons WHERE id = </span><span class="si">${</span><span class="nx">lessonId</span><span class="si">}</span><span class="sb"></span>
<span class="sb">    `</span><span class="p">)</span> <span class="kr">as</span> <span class="nx">LessonType</span><span class="p">[];</span>
<span class="p">});</span>
</pre></div>


<p>Now if you call <code>getLesson</code> on the client it sends the arguments to the server, runs it there, and returns the result (everything has to be JSONable, but I&#8217;ve been preferring &#8220;dead&#8221; objects<sup id="fnref:17"><a class="footnote-ref" href="#fn:17">14</a></sup> more and more anyway). On the server it acts like a normal&nbsp;function.</p>
<p>Actually handling the imports and calls is a bit of a pain, a bit of fighting with the bundler, but not much code and ultimately&nbsp;stable.</p>
<p>The <span class="caps">SQL</span> builder is inspired by <a href="https://vercel.com/docs/storage/vercel-postgres/sdk">Vercel&#8217;s <code>sql</code></a> and uses <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Template_literals">tagged templates</a>. It&#8217;s small but I keep futzing with it as I deal with little <span class="caps">SQL</span> situations. It doesn&#8217;t execute <span class="caps">SQL</span> immediately and so I can nest it&nbsp;like:</p>
<div class="highlight"><pre><span></span><span class="nx">dbExec</span><span class="p">(</span><span class="nx">sql</span><span class="sb">`</span>
<span class="sb">SELECT * FROM lessons</span>
<span class="sb">WHERE</span>
<span class="sb">  state = &#39;published&#39;</span>
<span class="sb">  </span><span class="si">${</span><span class="nx">titleLike</span> <span class="o">?</span> <span class="nx">sql</span><span class="sb">`AND title LIKE </span><span class="si">${</span><span class="nx">titleLike</span><span class="si">}</span><span class="sb">`</span> <span class="o">:</span> <span class="nx">sql</span><span class="sb">``</span><span class="si">}</span><span class="sb"></span>
<span class="sb">`</span><span class="p">);</span>
</pre></div>


<p>The resulting code reminds me a bit of <span class="caps">JSX</span>, for better or&nbsp;worse.</p>
<p>I&#8217;ve only made these additions to the stack recently, but I&#8217;m very happy with the result. When things are easy or hard it affects the paths I take, not just the speed of progress but also the kind of progress I&nbsp;make.</p>
<h2><span id="component-library">Component&nbsp;library</span></h2>
<p>I&#8217;ve tried and failed to use React component libraries. Frankly Copilot and Tailwind make it too easy to write my own. And everything works better when I use signals in the&nbsp;components.</p>
<p>An input field looks&nbsp;like:</p>
<div class="highlight"><pre><span></span>return (
    <span class="nt">&lt;div&gt;</span>
        <span class="nt">&lt;TextInput</span> <span class="na">label=</span><span class="s">&quot;Username&quot;</span> <span class="na">signal=</span><span class="s">{username}</span> <span class="nt">/&gt;</span>
        <span class="nt">&lt;TextArea</span> <span class="na">label=</span><span class="s">&quot;Bio&quot;</span> <span class="na">signal=</span><span class="s">{bio}</span> <span class="nt">/&gt;</span>
    <span class="nt">&lt;/div&gt;</span>
);
</pre></div>


<p>Now the session data is live-editable. This way I avoid most event handlers, instead exchanging&nbsp;signals.</p>
<p>I take good care of my <code>&lt;Markdown /&gt;</code> component because <span class="caps">GPT</span> loves Markdown (and LaTeX). I also create a lot of tabs on the page, which often get <em>very</em> nested. There&#8217;s a lot of information to make visible, and a lot of ways to present that information (rendered, source, diffs, code execution, tags, and more), which leads to a lot of nesting. I a shockingly complex <code>&lt;Button /&gt;</code> component which handles async onClick handlers, notification sounds for long events, and all sorts of other little&nbsp;niceties.</p>
<p><a href="../media/llm-dev-stack/screenshot-resultviewer.png" target="_blank"><img style="float: right; width: 30%; border-radius: 5px; margin-left: 4px;" src="../media/llm-dev-stack/screenshot-resultviewer.png" alt="Screenshot example of the result viewer" /></a></p>
<p>I have just a couple <span class="caps">LLM</span>-specific components. One views lists of <span class="caps">LLM</span> responses which allows comparing output, seeing the underlying prompt and response, and selecting the preferred response. Generating and displaying multiple responses is a feature I use but I&#8217;ve had a hard time getting collaborators to use. It&#8217;s a little too obscure, and a little too attached to the underlying pipeline and data design that I understand well and often no one else&nbsp;does.</p>
<p><a href="../media/llm-dev-stack/screenshot-prompt-editor.png" target="_blank"><img style="float: right; width: 30%; border-radius: 5px; margin-left: 4px;" src="../media/llm-dev-stack/screenshot-prompt-editor.png" alt="Screenshot example of the prompt editor" /></a></p>
<p>The other notable <span class="caps">LLM</span>-specific component is the prompt editor. Simple and a bit too information&nbsp;dense.</p>
<h2><span id="batch-processing">Batch&nbsp;processing</span></h2>
<p>I&#8217;ve been doing a lot of ad hoc processing, but it&#8217;s kind of a waste&#8230; this is automation after all, when we do things once we can do them a thousand&nbsp;times.</p>
<p>It was slow coming, but I&#8217;ve finally started working on these larger batches of work. The workflow is&nbsp;challenging&#8230;</p>
<ol>
<li>We t usuonally wat t start with applyie workfnglow to <em>everythin</em>.</li>
</ol>
<h2>App&nbsp;layout</h2>
<p>I put these pieces together in a kind of mini-application that is layed out like this (the arrows represent a dependency, e.g., <code>index.page.tsx</code> depends on <code>app.pipelines.ts</code>):</p>
<p><img style="width: 90%; max-width: 500px; padding: 5px; border-radius: 3px;" src="../media/llm-dev-stack/llm-deps.svg" alt="Module dependency diagram" /></p>
<h2><span id="not-implemented">Things that aren&#8217;t&nbsp;code</span></h2>
<p>I haven&#8217;t seen a reason to create libraries or special code for some things other <span class="caps">LLM</span> frameworks&nbsp;include:</p>
<h3><span id="pipelines">Pipelines</span></h3>
<p>There is nothing like a &#8220;pipeline&#8221; in the&nbsp;library.</p>
<p>I do conventionally make a module named &#8220;pipelines.ts&#8221;, which contains functions. The functions assemble and convert information, they call one or more prompts, and return or save the information. Occasionally they have loops (usually when I have to deal with partial output and retries). The functions are all hand-coded and usually quite&nbsp;short.</p>
<p>Creating pipelines is very important to getting good results from an <span class="caps">LLM</span>! I <em>usually</em> have multiple prompts with the output of one prompt feeding into another. But programming languages already have all the tools to implement pipelines: <a href="https://en.wikipedia.org/wiki/Structured_programming">structured programming</a>.</p>
<p>Because I allow selecting and running multiple prompt variants in parallel, the response from pipeline calls are always lists of responses. This is&#8230; awkward. Any place you have one response you might actually have many, and any time you pluralizing something it <em>makes programming hard</em>. I can&#8217;t decide if these plurality of responses are really worth it. But when I consider removing it I think: they <em>should</em> be worth it, because we <em>should</em> be doing more careful prompt&nbsp;comparisons.</p>
<h3><span id="rag">Retrieval Augmented Generation (<span class="caps">RAG</span>)</span></h3>
<p><a href="https://www.promptingguide.ai/techniques/rag"><span class="caps">RAG</span></a> is a set of techniques to include source information in your <span class="caps">LLM</span>&nbsp;prompts.</p>
<p>You can&#8217;t go very far with LLMs without giving them source information: facts, examples, references, and so on. Common sense knowledge only goes so far, and if that&#8217;s all you need then you might as well use the ChatGPT&nbsp;frontend.</p>
<p>Most of the time when I retrieve information to augment the generation I explicitly query the information<sup id="fnref:16"><a class="footnote-ref" href="#fn:16">15</a></sup>. In some sense if you are fetching the &#8220;appropriate&#8221; data from a database and inserting it into the prompt you have implemented &#8220;<span class="caps">RAG</span>&#8221;. I have periodically experimented with more implicit retrieval using <a href="https://platform.openai.com/docs/guides/embeddings/what-are-embeddings">embeddings</a> but most of the time I have found it both feasible and preferable to decide exactly what data you want to put into the&nbsp;prompt.</p>
<p>I also find it is very important to contextualize any information you give to the <span class="caps">LLM</span><sup id="fnref2:17"><a class="footnote-ref" href="#fn:17">14</a></sup>. Why is this data important, how does it relate to the task, how do different pieces of data relate to each other? It&#8217;s hard to contextualize if you have a pool of heterogeneous data. Once you&#8217;ve segmented the data into distinct types there&#8217;s often specific ways to fetch the appropriate data instead of fuzzy techniques like&nbsp;embeddings.</p>
<p>But even now I&#8217;m starting a phase of a project where I plan to use embeddings. I expect the code I write for this to look very imperative (i.e., not a&nbsp;framework).</p>
<p>If I was more serious about <span class="caps">RAG</span> I&#8217;d probably want a system to support multiple strategies<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">16</a></sup> in parallel for comparison, as well as tools to update and manage processes based on different&nbsp;strategies.</p>
<h3><span id="evaluation">Evaluation</span></h3>
<p>I mentioned that I allow for multiple versions of prompts and comparison between the output. But there&#8217;s no accompanying tools to compare entirely different pipelines, besides implementing those pipelines in parallel. I simply don&#8217;t have the time or focus on a singular pipeline to justify maintaining different pipelines for&nbsp;comparison.</p>
<p>I haven&#8217;t used general evaluation frameworks. The prompts are hand-crafted artisanal affairs. Development of prompts is usually concurrent with developing an idea of what exactly we want the prompt to do. Editorial rules have to become much clearer, and things that we&#8217;d previously call &#8220;good taste&#8221; have to be reified into instructions and pipelines. I don&#8217;t have reams of output and teams of people to rate that&nbsp;output.</p>
<p>In general I think working with an <span class="caps">LLM</span> is very amenable to <em>thinking</em> and <em>being clear</em>. It&#8217;s not a black box (at least if you can see the prompts). Machine Learning has taken on a strongly behavioral perspective: the only way to know something is to see its effect. That&#8217;s the wrong attitude for developing with an <span class="caps">LLM</span>.</p>
<h2><span id="what-doesnt-work-well">What doesn&#8217;t work well?<span></h2>
<p>Here are my biggest complaints (which I can only direct at&nbsp;myself):</p>
<ol>
<li>It&#8217;s hard to do large batches of work. Doing 1000 completions in the browser is a Bad Time.<ul>
<li>That said, the browser is a powerful execution&nbsp;environment.</li>
<li>State management and transfer is the hardest part of making this work, and I haven&#8217;t designed the system to work well with things like&nbsp;queues.</li>
<li>But I shouldn&#8217;t try that hard to fix it, instead I should be thinking of ways to take some of the small-batch work in the browser (such as prompt development) and easily rephrase it for large batch execution. It&#8217;s just a matter of&nbsp;work.</li>
</ul>
</li>
<li>It&#8217;s not particularly easy to repurpose this work for other environments, such as embedding functionality into a <span class="caps">CMS</span> or site.<ul>
<li>First: this is always&nbsp;hard.</li>
<li>I could &#8220;go native&#8221; and program it directly in one of these environments&#8230; but even at a modestly sized company like Brilliant there&#8217;s no <em>one</em>&nbsp;environment.</li>
<li>I could make it easier to turn functionality into an <span class="caps">API</span>, and then re-code the <span class="caps">UI</span> in a new environment once the prompts are working and validated, using the <span class="caps">API</span> as the <span class="caps">UI</span> backend. This shares a lot of work with batch&nbsp;processing.</li>
<li>I think we underestimate the overhead of separating <span class="caps">UI</span> and backend, maybe because it often means a split team where neither sees the total overhead and can retain a sense of mystery about what the other team&nbsp;does.</li>
<li>Alternately these other environments can grow APIs where my tools and <span class="caps">UI</span> can interact with them. This starts to feel a bit like a microservice, though a more &#8220;naked&#8221; microservice where the services have <span class="caps">UI</span> and the interfacing points may themselves be <span class="caps">UI</span> and not raw&nbsp;APIs.</li>
</ul>
</li>
<li>It&#8217;s hard to make the tools public.<ul>
<li>I have a few like the <a href="https://llm.ianbicking.org/"><span class="caps">LLM</span> Garden</a>, <a href="https://www.a-life-lived.com/">A Life Lived</a>, and <a href="https://www.worldwanderer.xyz/">World Wanderer</a>, but they all have issues and feel precarious. As far as I know no one really uses&nbsp;them.</li>
<li>Everything is built with public prompts in mind. I can (and sometimes do) hide the prompts in the <span class="caps">UI</span> to reduce the complexity, but you could find them just by opening a the browser&nbsp;console.</li>
<li>The browser requests completions directly from the <span class="caps">LLM</span> <span class="caps">API</span>. This makes key management quite hard. <span class="caps">LLM</span> calls are cheap enough for some things, but not open-it-up-to-the-internet&nbsp;cheap.</li>
<li>Key management is fine when building internal tools, I trust my fellow employees not to run up an <span class="caps">API</span> bill. (They all have the ability to run up bills&nbsp;somewhere.)</li>
<li>Those public sites all end up being bring-your-own-key. As long as the prompt is assembled on the client there&#8217;s no way to open up a gateway for prompt completion that isn&#8217;t also effectively an open&nbsp;proxy.</li>
</ul>
</li>
<li>It&#8217;s neither good nor bad at data intake and transformation<ul>
<li>Especially in my professional work a substantial portion of the work is pulling in the data necessary to properly instruct the <span class="caps">LLM</span>. Maybe&nbsp;50%?</li>
<li>It&#8217;s fine that this runs in the browser, and it&#8217;s not particularly bad at handling&nbsp;data</li>
<li>&#8230; but it&#8217;s not <em>great</em> at data intake. A system (library, component, environment, process) that was great at that would be&nbsp;superior.</li>
<li>Other frameworks often spend a lot of time talking about things like &#8220;document loaders&#8221; and that&#8217;s <em>not</em> what I mean. I don&#8217;t need to normalize the APIs for data access. I need patterns where the data feels like it&#8217;s right at my fingers, and where users also are able to bring in and manipulate data throughout the pipeline. I don&#8217;t even know what it should look like&#8230; AirTable? A data analog to a web browser? Something new that itself uses LLMs in key&nbsp;ways?</li>
</ul>
</li>
<li>Similar to data, there&#8217;s nothing to support spatial visualizations.<ul>
<li>I don&#8217;t want or believe in a drag-and-drop or visual editor. But it&#8217;s worth being able to <em>make</em> visualizations. (Maybe <a href="https://mermaid.js.org/">Marmaid</a> should become part of the&nbsp;toolbelt?)</li>
</ul>
</li>
</ol>
<div class="highlight"><pre><span></span>
</pre></div>


<div class="footnote">
<hr>
<ol>
<li id="fn:0">
<p>A lot of that work happened pretty early in the <a href="https://github.com/ianb/llm-garden"><span class="caps">LLM</span> Garden</a>, and then in a simplification/refactor in <a href="https://github.com/ianb/fungpt">FunGPT</a> that I used for a <a href="https://www.youtube.com/watch?v=qzczZABGzbQ">presentation</a>.&#160;<a class="footnote-backref" href="#fnref:0" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:1">
<p>There&#8217;s some cleverness to allow more permissive validation than the tool call specifies; LLMs take the <span class="caps">JSON</span> Schema as a suggestion, not a strict requirement, and sometimes that&#8217;s <span class="caps">OK</span> (for example, returning a number where a string is expected).&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>More completely the response contains the original prompt, <span class="caps">JSON</span> response, error messages, and information about the timing. It can both represent one response <span class="caps">JSON</span> with multiple choices when using the <a href="https://platform.openai.com/docs/api-reference/chat/create#chat-create-n"><code>n</code> parameter</a> or explode that into multiple response objects.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>If you look closely at some of these screenshots you&#8217;ll see some really naive prompts. These are examples! I would be embarrassed to write a prompt so naive, so I feel a need to protect my honor and that <em>you</em> know that <em>I</em> would not be happy with such a simplistic prompt.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p><span class="caps">OK</span>, I actually do start with simple prompts. It makes it much easier to find improvement! But also it gives a baseline: what does the <span class="caps">LLM</span> do with the most vague, hand-wavy instructions? What does it do acceptably without further instruction? Where is it lacking, in instruction, factual information, or context? What biases does the <span class="caps">LLM</span> bring that have to be specifically countered in the instructions?&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>The validation is done against a <a href="https://json-schema.org/"><span class="caps">JSON</span> Schema</a>, though at the moment it doesn&#8217;t <em>create</em> a <span class="caps">JSON</span> Schema for a given template but only matches against it. I have made some (questionable?) decisions to relax variable interpretation in the template language that don&#8217;t map well to a <span class="caps">JSON</span> Schema, especially implicitly turning <code>anArray.prop</code> into <code>anArray.map((item) =&gt; item.prop)</code>.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>I&#8217;ll use the term &#8220;Tool&#8221; consistently from here forward because it appears to be the new consensus name.&#160;<a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Tools work really well with <span class="caps">GPT</span> and Claude Sonnet 3.5, and terrible with everyone else (including Gemini). Most ways you use an <span class="caps">LLM</span> today are shockingly non-proprietary, but tools are something only OpenAI (and now maybe Anthropic) is good at. (Anyone else <em>could</em> be good at it, they just aren&#8217;t; OpenAI&#8217;s moat here is only as strong as the indifference of their competitors.)&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>It&#8217;s also possible to choose to get multiple copies of the same prompt. This is useful to test for <em>stability</em>: given the same input and prompt, how much will the output vary?&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>One missing feature: it <em>should</em> be possible to override the parameter order, but unfortunately I haven&#8217;t implemented that; order of generation is <em>very important</em> and it should be possible to experiment with different orders.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Realistically LLMs cannot take arbitrary <span class="caps">JSON</span> Schemas, for instance <code>type: ["number", "string"]</code> isn&#8217;t often supported, nor <a href="https://json-schema.org/understanding-json-schema/reference/combining">composition</a>. As a result the schemas you create are fairly simple, and can probably be described simply as a list of properties and their description, with small amount of metadata for type and arrays.&#160;<a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>My personal rule is that <code>role: system</code> is for general instructions, and that <code>role: user</code> is for the very specific action the <span class="caps">LLM</span> is being asked to do. Does it matter? I&#8217;m not even sure, maybe I could put everything in system. I almost never put unadorned text in the user message (unlike the example <code>{{userInput}}</code>), and I always contextualize any user input. Most pipelines I make have no user input.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:15">
<p>by a &#8220;box&#8221; I am referring to <a href="https://en.wikipedia.org/wiki/Boxing_(computer_science)">&#8220;Boxed values&#8221;</a>, though not in the way it&#8217;s used in Java (or JavaScript). Instead it means instead of passing around a specific value (for example, <code>func(10)</code>) we pass around a box (<code>func(countSignal)</code>) and use <code>countSignal.value</code> to get the value, and <code>countSignal.value = 11</code> to update the value. The &#8220;box&#8221; sticks around even while the value inside it changes. A simple object like <code>{value: 10}</code> is such a box, though Preact Signals do quite a bit more.&#160;<a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p><span class="dquo">&#8220;</span>contextualize&#8221; sounds fancy, but all I really mean is labeling the data in some fashion in the prompt. For instance: &#8220;Here are related definitions: {{definitions}}&#8221;&#160;<a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 14 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:17" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:16">
<p>to be clear, &#8220;querying the information&#8221; just means &#8220;load information like in any other program.&#8221; I may run a <span class="caps">SQL</span> query, parse some text with regexes to create subsets, or whatever, usually based on information the user provided directly such as indicating the lesson the user wants to work on.&#160;<a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>here a &#8220;strategy&#8221; would be a way to chop up the data, what text to put into the embedding (it doesn&#8217;t have to match the information that would be fetched by the embedding search), how many items to fetch, and so on.&#160;<a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
</ol>
</div></div>
        <!-- /.entry-content -->
  
        <hr />

        <!-- <div>
            Hello! Did you know as of December 2024 I'm looking for a job? <a href="https://www.linkedin.com/feed/update/urn:li:activity:7265435901009231872/">I am!</a> I really like working with LLMs, especially in the domain of education, wellness, and <a href="https://en.wikipedia.org/wiki/Executive_functions">executive function</a>. Maybe <a href="mailto:ianbicking@gmail.com">drop me an email</a>?
        </div> -->
    </article>
</section>
        <section id="extras" class="body">
                <div class="links">
                  <h2><a href="https://ianbicking.org">here</a></h2>
                  <ul>
                    <li><a href="/blog/">blog</a></li>
                    <li><a href="/projects.html">projects</a></li>
                    <li><a href="https://ianbicking.org/archives.html">archives</a> &amp; <a href="https://ianbicking.org/categories.html">categories</a></li>
                    <li><a href="https://ianbicking.org/category/ai.html">category: ai</a></li>
                    <li><a href="https://ianbicking.org/category/javascript.html">category: javascript</a></li>
                    <li><a href="https://ianbicking.org/category/misc.html">category: misc</a></li>
                    <li><a href="https://ianbicking.org/category/mozilla.html">category: mozilla</a></li>
                    </ul>
                </div>
                <div class="social">
                        <h2>elsewhere</h2>
                        <ul>
                            <li><a href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://hachyderm.io/@ianbicking">@ianbicking@hachyderm.io</a></li>
                            <li><a href="https://bsky.app/profile/ianbicking.org">Blue Sky</a></li>
                            <li><a href="https://www.threads.net/@ibicking">Threads</a></li>
                            <li><a href="https://github.com/ianb">Github</a></li>
                            <li><a href="https://www.linkedin.com/in/ianbicking/">LinkedIn</a></li>
                        </ul>
                </div><!-- /.social -->
                <div class="archives">
                  <h2><a href="https://ianbicking.org/blog/">recent posts</a></h2>
                  <ul>
                    <li><a href="https://ianbicking.org/blog/2025/07/intra-llm-text-adventure.html">Intra: design notes on an <span class="caps">LLM</span>-driven text&nbsp;adventure</a></li>
                    <li><a href="https://ianbicking.org/blog/2025/06/creating-worlds-with-llms.html">Roundup: Creating Worlds with&nbsp;LLMs</a></li>
                    <li><a href="https://ianbicking.org/blog/2025/05/the-hungry-ghost.html">The Hungry&nbsp;Ghost</a></li>
                    <li><a href="https://ianbicking.org/blog/2024/05/ai-aita.html"><span class="caps">AI</span> <span class="caps">AITA</span></a></li>
                    <li><a href="https://ianbicking.org/blog/2024/04/roleplaying-by-llm.html">Roleplaying driven by an <span class="caps">LLM</span>: observations <span class="amp">&amp;</span> open&nbsp;questions</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/04/world-building-gpt-2-declarative.html">World Building with <span class="caps">GPT</span> part 2: bigger, better, more&nbsp;declarative</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/02/world-building-with-gpt.html">World Building With <span class="caps">GPT</span></a></li>
                    <li><a href="https://ianbicking.org/blog/2023/01/thoughts-on-voice-interfaces-2-llms.html">Thoughts On Voice Interfaces 2 years later:&nbsp;LLMs</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/01/infinite-ai-array.html">Infinite <span class="caps">AI</span>&nbsp;Array</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/11/firefox-was-always-enough.html">Firefox Was Always&nbsp;Enough</a></li>
                  </ul>
                </div><!-- /.archives -->

        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
          This is the personal site of <a href="/">Ian Bicking</a>.  The opinions expressed here are my own.
        </footer><!-- /#contentinfo -->

<script src="/theme/instantclick.min.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FKT4HDGBE4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FKT4HDGBE4');
</script>
        </div><!-- /#main-container -->
        </div><!-- /#main-wrapper2 -->
        </div><!-- /#main-wrapper1 -->
</body>
</html>