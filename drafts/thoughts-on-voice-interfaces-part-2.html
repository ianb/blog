<!DOCTYPE html>
<html lang="en">
<head>
        <title>Thoughts On Voice Interfaces Part 2</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
          <link rel="stylesheet" href="https://ianbicking.org/theme/css/style.min.css">
        <!--<link rel="stylesheet" href="https://ianbicking.org/theme/css/main.css" type="text/css" />-->
        <link href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="Ian Bicking: a blog Atom Feed" />
        <link rel="icon" href="https://ianbicking.org/favicon.ico">

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="https://ianbicking.org/css/ie.css"/>
                <script src="https://ianbicking.org/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="https://ianbicking.org/css/ie6.css"/><![endif]-->

</head>

<body id="index" class="home">
  <div id="main-wrapper1">
  <div id="main-wrapper2">
  <div id="main-container">
        <header id="banner" class="body">
                <h1><a href="https://ianbicking.org">Ian Bicking: a blog </a></h1>
        </header><!-- /#banner -->
<section id="content" class="body">
    <article>
        <header        style="background-image: url(https://ianbicking.org/media/cover-images/talking-with-a-robot.png); background-size: cover; background-position: center; height: 300px; width: 100%;"
>
        <div        style="background-color: rgba(255, 255, 255, 0.7)"
>
            <div class="published" title="2023-01-02T00:00:00-06:00">
                Monday, January 2nd, 2023
            </div>
            <h1 class="entry-title">
                <a
                    href="https://ianbicking.org/drafts/thoughts-on-voice-interfaces-part-2.html"
                    rel="bookmark"
                    title="Permalink to Thoughts On Voice Interfaces Part 2"
                    >Thoughts On Voice Interfaces Part&nbsp;2</a
                >
            </h1>
        </div>
        </header>

        <div class="entry-content"><p>Over two years ago I wrote <a href="https://ianbicking.org/blog/2020/08/thoughts-on-voice-interfaces.html">Thoughts On Voice Interfaces</a>, a scattering of ideas about voice interactions. Time has passed, I&#8217;ve worked more on voice interfaces, and I&#8217;d like to update that with <span class="caps">NEW</span> <span class="caps">THOUGHTS</span>:</p>
<ol>
<li>In two years <em>very</em> little has changed. The big three assistants (Google, Apple/Siri, Amazon/Alexa) have barely&nbsp;changed.</li>
<li>With the addition of Large Language Models (LLMs), especially <span class="caps">GPT</span>-3, I believe everything will&nbsp;change </li>
<li>But not&nbsp;quickly</li>
<li>Existing systems are based on categorization and entity extraction:<ol>
<li>You have a bunch of commands, the commands take certain kinds of arguments (song title, time span, search query,&nbsp;etc).</li>
<li>Any utterance is compared to examples and categorized as a command, and the arguments (&#8220;entities&#8221;) are&nbsp;extracted.</li>
<li>This creates the basic shape of voice interactions. This command-based model is not begging for LLMs, though they can certainly increase accuracy in some&nbsp;cases.</li>
<li>Though I&#8217;m assuming that Whisper achieves its impressive accuracy because of <span class="caps">LLM</span>-like techniques that interpret sound using a broad knowledge of language and&nbsp;ideas.  </li>
</ol>
</li>
<li>If you plug voice into a <span class="caps">GPT</span>-3-driven interaction you&#8217;ll find it&#8217;s very sensitive to speech recognition inaccuracies.<ol>
<li>In the traditional categorization model it will naturally ignore many inaccuracies, because those inaccuracies will point to something that&#8217;s not among its&nbsp;commands.</li>
<li>That is: the traditional model tries very hard to keep the conversation on the rails, and one inaccuracy won&#8217;t make you jump to another&nbsp;rail.</li>
<li>The beauty of an <span class="caps">LLM</span> is that we might not need rails, that we can make vastly more capable agents that can operate on a much wider set of&nbsp;input.</li>
<li>The danger of LLMs is they take things very literally, and are very credulous. You want to know <em>what&#8217;s next on my caliper</em>? It will try very hard to come up with a creative answer instead of figuring out you were asking about your&nbsp;calendar.</li>
<li>I&#8217;m sure this isn&#8217;t a fatal flaw, but it&#8217;s a challenge right&nbsp;now.</li>
</ol>
</li>
<li>LLMs are stereotyped as text <em>generators</em>, but they are also great at <em>understanding</em>.<ol>
<li>While there&#8217;s lots of speech or text output opportunities, LLMs can be used successfully with non-text output as&nbsp;well.</li>
</ol>
</li>
<li>Two years ago I complained there&#8217;s no &#8220;understanding&#8221; in &#8220;natural language understanding&#8221;. Now that can change.<ol>
<li>It&#8217;s still not real &#8220;understanding&#8221; when an <span class="caps">LLM</span> responds to something; there isn&#8217;t an entity underneath that is understanding&nbsp;things.</li>
<li>The understanding is when we fuse that human input with meaningful output or&nbsp;action.</li>
<li>At least something that <em>looks like</em> understanding, I&#8217;m not trying to make bigger claims. I believe adding this action output is referred to as a &#8220;grounded&#8221;&nbsp;model.</li>
<li><a href="https://en.wikipedia.org/wiki/Prompt_engineering">Prompt Engineering</a> is an important part of that fusion: adding context to the input and interpreting the output. These shells we create are an embodiment of <span class="caps">AI</span>, what turns a mechanism (an <span class="caps">LLM</span>) into a meaningful&nbsp;entity.</li>
<li>Some understanding will be inference: being able to contextualize and make reasonable assumptions about what the human means. This is what people pay a lot of attention to when talking about &#8220;understanding,&#8221; though I think it&#8217;s&nbsp;over-emphasized.</li>
<li>Much of understanding will be translation: taking our speech and translating it into an actionable&nbsp;form.</li>
<li>We see this currently in <span class="caps">GPT</span>&#8217;s code generation. You can ask <span class="caps">GPT</span> for concrete answer but it&#8217;s also very good at writing down the steps by which you can get an&nbsp;answer.</li>
<li><span class="dquo">&#8220;</span>The steps by which you can get an answer&#8221; is another way of saying &#8220;a program&#8221;. We will be programming our agents and getting our agents to generate&nbsp;programs.</li>
<li>You won&#8217;t see the programs of course. Mostly. Interestingly <span class="caps">GPT</span> is also pretty good at turning code into natural language. I expect this round-tripping will be useful in debugging our agents, something we&#8217;ll probably have to do individually from time to time. But like we ask another person to repeat our instructions to make sure we were clear, we may ask our <span class="caps">AI</span> agents to do the&nbsp;same.</li>
<li>What&#8217;s the programming language that our personal agents will be generating based on our commands?<ol>
<li>Probably something new, and probably&nbsp;accidental.</li>
</ol>
</li>
</ol>
</li>
<li>A mind-bending part of building a chat interface directly on <span class="caps">GPT</span>-3 is that you have to create archetypes and entities from scratch.<ol>
<li>There is no &#8220;you&#8221; and &#8220;me&#8221;. &#8220;You&#8221; is a character that has to be introduced and described to the <span class="caps">LLM</span>. Armed with a description the <span class="caps">LLM</span> will then predict what this entity might do. The <span class="caps">LLM</span> doesn&#8217;t &#8220;pretend&#8221; to be &#8220;you&#8221; as there is no underlying personality. And &#8220;me&#8221; (the human) is again an introduced entity. Without constraints the <span class="caps">LLM</span> will gladly predict <em>both</em> sides of the&nbsp;conversation.</li>
<li>Maybe there&#8217;s something cool that could be done by making it a conversation between me and me: that is, modeling the interaction on internal&nbsp;dialog.</li>
</ol>
</li>
<li>It&#8217;s a fun party trick to make <span class="caps">GPT</span>-3 say things in the style of <a href="https://twitter.com/tqbf/status/1598513757805858820">The Bible</a> or <a href="https://twitter.com/goodside/status/1562991379915341824">Jar Jar Binks</a>, but it&#8217;s more than that&#8230;<ol>
<li>Asking <span class="caps">GPT</span> to write something for a kindergartener is great. Maybe that should even be a more standard starting point than &#8220;normal&#8221;&nbsp;output.</li>
<li>Asking it to write for speech output would also be good. I haven&#8217;t had much success in my brief experiments, I don&#8217;t know what to ask for, and I&#8217;m not sure what &#8220;good for speech output&#8221; really means, but there must be something&nbsp;there.</li>
<li>All of this is also very amenable to user override: changing the tone to be more brief, emphasize certain points,&nbsp;etc.</li>
<li>Given the right chat context for an individual, and enough quantity, I&#8217;m guessing an <span class="caps">LLM</span> might almost naturally start to adopt the human&#8217;s phrasing and language&nbsp;style.</li>
<li>Though I wonder if people naturally <a href="https://en.wikipedia.org/wiki/Code-switching">code switch</a> to speak in a &#8220;formal&#8221; style with voice interfaces. (&#8220;Naturally&#8221; may be learned behavior to avoid speech recognition errors, or it might also be based on the mental model we have about our relationship with the <span class="caps">AI</span>&nbsp;agent.)</li>
</ol>
</li>
<li>I&#8217;m optimistic about the novel way you can compose text in ChatGPT, by acting more as an editor than an author, and how this will be applicable to voice&#8230;<ol>
<li>If ChatGPT writes something for you and you want to change it, the best way is usually to ask for the change. &#8220;Make it shorter,&#8221; or &#8220;take out the part about Jar Jar&nbsp;Binks.&#8221;</li>
<li>This is also a good way to edit with your voice. Referring to specific sentences or paragraphs or describing mechanical changes is very hard to do with voice. It&#8217;s hard to compose those edits in your head, and it&#8217;s hard to say them&nbsp;accurately.</li>
<li><span class="caps">GPT</span> doesn&#8217;t have to generate the text in order to be able to edit&nbsp;it.</li>
</ol>
</li>
<li>I think there are entirely new patterns of interaction possible with LLMs<ol>
<li>Instead of commanding, maybe you speak to your <span class="caps">AI</span> agent freely; the emphasis is just to dump a lot of stuff into&nbsp;it</li>
<li>Then you start using that accumulated information as the basis for&nbsp;action</li>
<li>Maybe it&#8217;s less transactional, less modal. You could be asking it to do something and then instead of pausing and waiting for it to do the thing you could just&#8230; keep going. The <span class="caps">AI</span> starts to assemble a plan for complicated actions, while still immediately acting on things that are easy and low-impact, though also respecting any higher-level instructions like ordering or&nbsp;conditionals.</li>
</ol>
</li>
<li>Follow-up and refining commands will be much easier with LLMs<ol>
<li>This isn&#8217;t just the continuity &#8220;chatbots,&#8221; but also for functional&nbsp;purposes</li>
<li>This represents an alternative to how we usually think about &#8220;learning&#8221; in voice interfaces, which is about frequency and probabilities. There&#8217;s a lot of ideas about how to predict what the user will do and then bias the result based on that, or change the interface to make the likely thing easier, or even to do something proactively for the user. This is not&nbsp;that!</li>
<li>Armed with many predictive capabilities I think we&#8217;ve been projecting prediction onto human-to-human interactions where it doesn&#8217;t exist. Your favorite barista does not actually make the coffee before you arrive. And even if they ask &#8220;the regular?&#8221; they are picking up the thread of past interactions and past conversations, not just predicting what you&#8217;ll have. Prediction removes the autonomy and self-direction of the user. If there&#8217;s a predictive analog in human-to-human interaction it&#8217;s probably in how we stereotype each other and our respective roles, not in how we&nbsp;communicate.</li>
<li>But LLMs <em>love</em> context. In your conversation you&#8217;re building a transcript. It doesn&#8217;t stop you from wandering, but it&#8217;s always ready to build off that previous&nbsp;context.</li>
<li>When to erasing context and forget will probably be an important implementation detail. (How well will an <span class="caps">LLM</span> be able to handle context that&#8217;s tagged as &#8220;maybe the human remembers this, maybe&nbsp;not&#8221;?)</li>
</ol>
</li>
<li>A &#8220;prompt&#8221; isn&#8217;t just a command for an <span class="caps">LLM</span>, it&#8217;s context<ol>
<li>We haven&#8217;t been providing this context to intent parser or to speech recognition itself. That will have to&nbsp;change.</li>
<li>Consider a case like sending a message to a contact: that often involves the difficult vocabulary of people&#8217;s names. We may solve this generally by personalizing models so they work better with personalized information like your list of contacts. But with an <span class="caps">LLM</span> we can include that information in the prompt or context, much more casually and contextually than most&nbsp;personalization.</li>
</ol>
</li>
<li><span class="caps">GPT</span> and probably all LLMs are fairly expensive, more expensive than any of the current techniques.<ol>
<li>Hopefully this stuff gets cheaper, not because compute gets much cheaper but because smart people just figure things out. But there may be a hard lower bound on the&nbsp;cost.</li>
<li>It&#8217;s possible to select different models and approaches in different contexts, most of them more efficient; I expect very eclectic stacks to&nbsp;emerge.</li>
</ol>
</li>
<li>When we start getting Large Language Models built on <a href="https://twitter.com/ianbicking/status/1612944221199294464">predicting text <span class="caps">AND</span> behavior</a> I think we&#8217;ll see another burst of&nbsp;capability.</li>
<li>So obviously I&#8217;m personally pretty focused on the effect of LLMs, but there&#8217;s more to be&nbsp;said&#8230;</li>
<li>Endpointing remains a major issue. That is, deciding when the human is &#8220;done&#8221; speaking and then acting on that&nbsp;speech.</li>
<li>The idea of &#8220;done&#8221; itself imposes very specific ideas on how an interaction&nbsp;works.</li>
<li>This whole &#8220;done&#8221; thing is imposed. Every system that uses silence-based endpointing is also capable of listening a little longer, with no real privacy issue. It feels a bit like a &#8220;lalala I&#8217;m not listening to you&#8221; situation, where it&#8217;s better just to not know that the user is continuing to&nbsp;speak.</li>
<li>But <span class="caps">TTS</span> output <em>does</em> make it difficult. With the right physical device setup I assume it would be possible to quickly stop <span class="caps">TTS</span> when the original speaker continues to speak. Thoroughly designed vertically integrated physical devices are not the norm, so this pattern doesn&#8217;t become the norm.<ol>
<li>I feel a sense of both optimism and disappointment about a well integrated physical device; that is a device where the microphone, speaker, physical controls, other input modalities (like an <a href="https://en.wikipedia.org/wiki/Inertial_measurement_unit"><span class="caps">IMU</span></a>), speech recognition, speech understanding, command execution, and output systems are all integrated for a more ideal experience. The potential is there at every level but hardware like this requires capital and the total experience requires vision. Will we have to wait for a big player to discover the vision, or for the hardware design to become more accessible for a new entrant? (I wish <a href="https://www.ray-ban.com/usa/ray-ban-stories">Rayban Stories</a> were a more open&nbsp;environment.)</li>
</ol>
</li>
<li>Between humans we have many forms of non-verbal or at least non-speech ways of indicating a desire to continue or interrupt. This is also a point of frequent errors, and requires reading other people in a way that many people find difficult. Still I think there&#8217;s more room to mimic some of these patterns. The &#8220;uh&#8221; call to attention, the stare-at-the-sky please-wait gesture, the facial expression of pausing&nbsp;mid-sentence.</li>
<li>I&#8217;m excited about the possibility for asymmetric I/O: speaking into the <span class="caps">AI</span>, but presenting the results&nbsp;visually.</li>
<li>This would solve some of the all-or-nothing issue with intent parsing. It&#8217;s much easier to live update something visual, and so the computer can show its best-effort interpretation at any&nbsp;time.</li>
<li>Screens represent a conversation between the human and the computer. The computer is showing what is possible (buttons and fields), what it knows (informational elements), the status of different inputs (cursors, dialog boxes). The standard <span class="caps">GUI</span> conversation with a computer is more like whiteboarding than a conversation.<ol>
<li>Just doing voice control of our screens certainly hasn&#8217;t caught on. But these traditional <span class="caps">GUI</span> elements – buttons, etc – are specific to non-voice inputs. What would a <span class="caps">GUI</span> designed voice input look like? I genuinely don&#8217;t&nbsp;know!</li>
<li>When phrased as an accessibility technology voice will be caught in this not-design-for-voice trap. &#8220;Accessibility&#8221; implies &#8220;small group of people,&#8221; and &#8220;not your real audience,&#8221; and &#8220;applicable to status quo&nbsp;software.&#8221;</li>
</ol>
</li>
<li>There&#8217;s room for more voice consumption that doesn&#8217;t have any output at all, or immediately do&nbsp;anything</li>
<li>That isn&#8217;t <em>just</em> voice recorders. There are tools like Otter or Google Recorder that faithfully listen, but mostly just&nbsp;transcribe.</li>
<li>For instance I&#8217;ve imagined a process for recording family photos and documents with voice and photos, so that you&#8217;re photos (of pictures, etc) are connected with the stories you can tell about that photo?<ol>
<li>Maybe there&#8217;s a generalization that you can fuse voice with other items if you have a <em>record</em> of those things, so that you can fix mistakes later instead of relying on live feedback to fix mistakes during the&nbsp;interaction.</li>
</ol>
</li>
<li>Invocation remains a barrier to fluid voice&nbsp;interactions</li>
<li>Wake words and other voice invocations like special hardware gestures are only accessible when you have control of the <span class="caps">OS</span>.</li>
<li>There aren&#8217;t many tools to prototype wake words or keyword spotting.<ol>
<li>This also makes it hard to make use of procedure&nbsp;words</li>
</ol>
</li>
<li>Whisper is impressive but not real&nbsp;time</li>
<li>Right now speech recognition systems usually have <em>partial</em> and <em>final</em> transcriptions. Everyone has seen those partial transcriptions where any word can change as you talk. These represent two levels of recognition. Whisper uses an even wider window of analysis to identify a sensible sentence given the input. Could we simply have three levels: very fast partial transcriptions, 1 second transcriptions, and then a 4 second &#8220;this time it&#8217;s really final&#8221; transcript? But seeing the transcript update even further back may be confusing or distracting for the user, and it may be hard to trust that errors in the not-quite-final transcription will get fixed with a little patience. Also hypothesis transcriptions already make this stuff hard to&nbsp;code!</li>
<li>Whisper makes retraining seem more accessible. Are there alternatives to current microphones that get enough information to create a transcription, even if they don&#8217;t get enough information to build an audio recording that other people would recognize? Like a microphone in earbuds, embedded in eyeglass temples, or something touching the nose bridge or neck&#8230; and instead of using tricks to make those microphones sound &#8220;normal,&#8221; train directly on the raw data they&nbsp;produce.</li>
<li>The benefit of wide-window speech recognition is also that you have a lot of context. That context isn&#8217;t just sounds, or general personalization, it&#8217;s your recent history, any modes or recent output. (Are there any speech recognition systems that let you pass in recent&nbsp;context?)</li>
</ol>
<p>Comments welcome on <a href="">Mastodon</a>, <a href="">Twitter</a>.</p>
<p>If you&#8217;ve gotten this far I will also throw in here that I (<a href="https://ianbicking.org">Ian</a>) am looking for a job, and maybe the best job for me is one that I don&#8217;t yet know exists. I&#8217;m particularly interested in the area of large language models, new user interactions built on LLMs (especially their abilities to understand us in new ways). I&#8217;m excited about education, aiding in executive function, and human-centered interactions. <a href="mailto:ianbicking@gmail.com">Let me know if you have ideas</a>, I would appreciate&nbsp;it!</p>
<p>The&nbsp;end.</p>
<ul>
<li>Multiple ways of summarizing things with LLMs are going to be helpful. Also &#8220;what&nbsp;happened&#8221;</li>
<li>Education:&nbsp;opportunities</li>
<li>What does it mean when an interaction isn&#8217;t&nbsp;imperative?</li>
<li>What has changed from the last list?&nbsp;https://ianbicking.org/blog/2020/08/thoughts-on-voice-interfaces.html</li>
<li>Leading with nouns or verbs (we always lead with verbs right&nbsp;now)</li>
<li>Is there any room for multiple personalities? Can they represent different contexts, patterns,&nbsp;jobs?</li>
<li>Maybe we can make up for the lack of action abilities in an assistant by also doing queuing. If you can&#8217;t do something, help the person remember to do it when it&#8217;s&nbsp;appropriate.</li>
<li>The outlining I mentioned before – speaking intentions before speaking the actions to achieve that intention – are going to be super-powered by&nbsp;LLMs.</li>
<li>Maybe we can make auto-filling the outline part of our&nbsp;experiences</li>
</ul></div>
        <!-- /.entry-content -->
      </article>
</section>
        <section id="extras" class="body">
                <div class="links">
                  <h2><a href="https://ianbicking.org">here</a></h2>
                  <ul>
                    <li><a href="/blog/">blog</a></li>
                    <li><a href="/projects.html">projects</a></li>
                    <li><a href="https://ianbicking.org/archives.html">archives</a> &amp; <a href="https://ianbicking.org/categories.html">categories</a></li>
                    <li><a href="https://ianbicking.org/category/ai.html">category: ai</a></li>
                    <li><a href="https://ianbicking.org/category/javascript.html">category: javascript</a></li>
                    <li><a href="https://ianbicking.org/category/misc.html">category: misc</a></li>
                    <li><a href="https://ianbicking.org/category/mozilla.html">category: mozilla</a></li>
                    </ul>
                </div>
                <div class="social">
                        <h2>elsewhere</h2>
                        <ul>
                            <li><a href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://twitter.com/ianbicking">@ianbicking</a></li>
                            <li><a href="https://hachyderm.io/@ianbicking">@ianbicking@hachyderm.io</a></li>
                            <li><a href="https://github.com/ianb">Github</a></li>
                        </ul>
                </div><!-- /.social -->
                <div class="archives">
                  <h2><a href="https://ianbicking.org/blog/">recent posts</a></h2>
                  <ul>
                    <li><a href="https://ianbicking.org/blog/2023/01/infinite-ai-array.html">Infinite <span class="caps">AI</span>&nbsp;Array</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/11/firefox-was-always-enough.html">Firefox Was Always&nbsp;Enough</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/09/project-ideas-2020.html">Project ideas for (what&#8217;s left of)&nbsp;2020</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/09/a-history-of-projects.html">A History Of&nbsp;Projects</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/08/thoughts-on-voice-interfaces.html">Thoughts on Voice&nbsp;Interfaces</a></li>
                    <li><a href="https://ianbicking.org/blog/2019/07/kling-axes-of-politics-technocrats.html">Kling&#8217;s Axes of Politics, and the&nbsp;Technocrats</a></li>
                    <li><a href="https://ianbicking.org/blog/2019/04/users-want-control-is-a-shrug.html"><span class="dquo">&#8220;</span>Users want control&#8221; is a shoulder&nbsp;shrug</a></li>
                    <li><a href="https://ianbicking.org/blog/2019/03/open-source-doesnt-make-money-by-design.html">Open Source Doesn&#8217;t Make Money Because It Isn&#8217;t Designed To Make&nbsp;Money</a></li>
                    <li><a href="https://ianbicking.org/blog/2019/03/firefox-experiments-i-would-have-liked.html">The Firefox Experiments I Would Have Liked To&nbsp;Try</a></li>
                    <li><a href="https://ianbicking.org/blog/2019/01/overengaged-knowledge-worker.html">The Over-engaged Knowledge&nbsp;Worker</a></li>
                  </ul>
                </div><!-- /.archives -->

                <div class="widgets">
                  <h2><a href="https://twitter.com/ianbicking">tweets</a></h2>
                  <a class="twitter-timeline" width="230" height="500" href="https://twitter.com/ianbicking" data-widget-id="319849964417187842" data-link-color="#aaae94">Tweets by @ianbicking</a>
<script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");</script>

                </div><!-- /.widgets -->

        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
          This is the personal site of <a href="/">Ian Bicking</a>.  The opinions expressed here are my own.
        </footer><!-- /#contentinfo -->

<script src="/theme/instantclick.min.js"></script>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-2442258-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
        </div><!-- /#main-container -->
        </div><!-- /#main-wrapper2 -->
        </div><!-- /#main-wrapper1 -->
</body>
</html>