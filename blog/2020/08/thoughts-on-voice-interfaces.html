<!DOCTYPE html>
<html lang="en">
<head>
        <title>Thoughts on Voice Interfaces</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1" />
          <link rel="stylesheet" href="https://ianbicking.org/theme/css/style.min.css">
        <!--<link rel="stylesheet" href="https://ianbicking.org/theme/css/main.css" type="text/css" />-->
        <link href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate" title="Ian Bicking: a blog Atom Feed" />
        <link rel="icon" href="https://ianbicking.org/favicon.ico">

</head>

<body id="index" class="home">
  <div id="main-wrapper1">
  <div id="main-wrapper2">
  <div id="main-container">
        <header id="banner" class="body">
                <h1><a href="https://ianbicking.org">Ian Bicking: a blog </a></h1>
        </header><!-- /#banner -->
<section id="content" class="body">
    <article>
        <header>
        <div>
            <div class="published" title="2020-08-03T00:00:00-05:00">
                Monday, August 3rd, 2020
            </div>
            <h1 class="entry-title">
                <a
                    href="https://ianbicking.org/blog/2020/08/thoughts-on-voice-interfaces.html"
                    rel="bookmark"
                    title="Permalink to Thoughts on Voice Interfaces"
                    >Thoughts on Voice&nbsp;Interfaces</a
                >
            </h1>
        </div>
        </header>

        <div class="entry-content"><p>I&#8217;ve been working on the Consumer Voice Products team in Mozilla for about a year now. My primary project has been <a href="https://github.com/mozilla-extensions/firefox-voice">Firefox Voice</a>, but our mandate is&nbsp;larger.</p>
<p>I still feel like a beginner in the area of voice, but I have developed some opinions. Many observations are the influence of other people’s ideas, work, or research, but I’ve lost the provenance. I’ve benefited a great deal from the long and regular discussions I’ve had with my team, particularly <a href="http://abewallin.com">Abraham Wallin</a>, <a href="http://www.harraton.com/">Janice Tsai</a>, <a href="http://jofish.com">Jofish Kaye</a>, and <a href="https://juliacambre.com">Julia Cambre</a>.</p>
<p><strong>Update:</strong> <a href="https://voicebot.ai/2020/09/07/ian-bicking-talks-firefox-voice-and-observations-about-assistants-today-voicebot-podcast-ep-166/">Hear me talk about some of these topics on this Voicebot podcast&nbsp;episode</a></p>
<ol>
<li>Voice interfaces are voice <em>interfaces</em>. They are a way for the user to express their desire, using patterns that might be <a href="https://en.wikipedia.org/wiki/Skeuomorph">skeuomorphism</a> of regular voice interactions, or might be specific learned behaviors. It&#8217;s not a conversation. You aren&#8217;t talking with the computer.<ol>
<li>I suspect you can push the user into a conversational skeuomorphism if you think that&#8217;s best, and the user will play along, but it&#8217;s no more right than another metaphor. It&#8217;s a question of quality of interaction, not ease or&nbsp;familiarity.</li>
<li>That said, speaking is an improvisation. You have something you want to say, you&#8217;ve probably prepared a few keywords, but the rest you make up word-by-word. Words won’t come out your mouth with grammatical&nbsp;precision.</li>
</ol>
</li>
<li>I hate how voice interfaces force us to speak without pauses because a pause is treated as the end of the statement. Firefox Voice does the same thing, so I appreciate why it works this way. I still hate it.<ol>
<li>Typically tools operate on the granularity of an <a href="https://en.wikipedia.org/wiki/Utterance">utterance</a>: one statement, command, query. It&#8217;s the same for conversational interfaces, just a different kind of&nbsp;turn-taking.</li>
<li>Many systems use a &#8220;wakeword&#8221; or &#8220;keyword spotting&#8221; to start the interaction. What if we used keyword spotting to determine the end as well? &#8220;Please&#8221; might be a good choice. It&#8217;s like the Enter&nbsp;key.</li>
<li>I read a science fiction story where they used voice interfaces, and used &#8220;please&#8221; as a statement terminator as well (in a <a href="https://archive.org/details/Asimovs_v15n12n13_1991-11/page/n227/mode/2up">story by Phillip C. Jennings</a> <a href="https://archive.org/stream/Asimovs_v15n12n13_1991-11/Asimovs_v15n12n13_1991-11_djvu.txt">[text only]</a> <a id="footnote1-source" href="#footnote1">**</a>). This also made &#8220;please&#8221; insulting, using it meant you regarded someone as no more important than a&nbsp;computer.</li>
<li>Another option might be speculative execution while allowing amendments. A common example would be when you want to say &#8220;set reminder for 2pm to go to the post office&#8221; but you paused just a little too long and the assistant jumped on you at &#8220;set reminder for 2pm&#8221;. Now it&#8217;s going to blather on about a question (“what reminder?”), and not only do you have to wait, you have to time your response. It would be nice if the moment you said &#8220;set reminder for 2pm&#8221; the assistant would indicate (visually) &#8220;setting 2pm reminder for what?&#8221; and you might answer anytime, and if you were slow it might give an audio prompt which you could also talk&nbsp;over.</li>
</ol>
</li>
<li>Some people think it is important that we <a href="https://www.the-vital-edge.com/digital-assistants-abuse/">not abuse assistants</a>. They believe abuse will make us cold or abusive to each other. I do not agree.<ol>
<li>We&#8217;ve also learned that people feel embarrassed when they can&#8217;t get their assistant to understand them. It&#8217;s different than other interfaces, as the voice makes it feel more personal and judgy. So concern over emotional impact is not&nbsp;misplaced.</li>
<li>When I used a <span class="caps">GPS</span> in the car regularly it would continually give voice instructions. I&#8217;d miss a turn and it would constantly tell me to make a U-turn or otherwise backtrack. I&#8217;d also just <em>choose</em> another route and it would complain. I knew I didn’t make a mistake, but I still felt judged. Then I turned off the voice and it was fine, the screen just informed me, it didn&#8217;t judge&nbsp;me.</li>
<li>Which is to say: I don’t think the answer is compassion towards our computers. They neither need it nor even ask for it. Instead of navigating through the uncanny valley we should keep computer and human separate. You want to cut short what your assistant is saying? Please do. Our challenge as implementers is to keep your heart from going cold by making it very clear this isn&#8217;t a human and has no&nbsp;feelings.</li>
<li>An aside, but <a href="https://www.youtube.com/watch?v=rVlhMGQgDkY">this classic video</a> of people taunting a Boston Robotics robot made people uncomfortable (including me!). They are being such jerks! I think the answer is that we should not make robots that look like humans. You and I pass on the sidewalk, and we have to navigate cooperatively to keep from hitting each other. If I bust through you paying no attention that makes me the asshole. I don&#8217;t want to have to start navigating politeness with robots too: they should step aside; if I place a hand on a robot it should stop, not be offended. I&#8217;m not polite to doors either, and this does not make me a worse&nbsp;person.</li>
</ol>
</li>
<li>Voice is not a command-line interface. Voice ends up as text, and despite caveats&#8230; really it’s just treated as text. So it seems appealing to treat it like a command line interface, but no&#8230;<ol>
<li>Accuracy is a big issue. Transcription errors are common. You can recover from a lot, but I think it puts a real upper limit on how much information you can give the computer before interaction is required. Maybe you are 95% successful saying one thing. But then you only have 90% chance of saying two concepts together. If the concept is complicated then 95% accuracy is&nbsp;generous.</li>
<li>If you execute something complicated using several smaller commands then you have opportunities to fix problems part way through. You&#8217;ll need those opportunities. We call this&nbsp;“repair”.</li>
<li>I think &#8220;undo&#8221; would be a nice capability to build into everything. It would probably be a prefix to your reparative command. Like, &#8220;no, search for nearby tacos&#8221; meaning undo last command and then do this new&nbsp;one.</li>
<li>Besides accuracy, it&#8217;s also mentally harder to successfully compose complex or precise sentences when speaking. Tools can ignore uninteresting words, accept multiple phrasings, put in reasonable defaults, but we can only do that once you’ve spoken. We can&#8217;t boost your brain to make it easier to speak complex&nbsp;phrases.</li>
<li>GUIs are a little like a discussion. You get a menu of options – buttons and controls – and micro-feedback like hover states or a depressed state, as well as macro-feedback like actual changes in the screen to indicate what happened. Trying to compose a compound voice statement can be trying to interact with a laggy <span class="caps">UI</span> where you outrun the screen refresh. It&#8217;s frustrating by&nbsp;default.</li>
<li>It&#8217;s not necessarily easy to give humans compound commands either. Try accomplishing something by looking over someone&#8217;s shoulder and telling them what to&nbsp;click&#8230;</li>
<li>Familiarity makes it possible to talk through a task with both computers and humans. I&#8217;ve definitely become familiar with my Alexa, I&#8217;ve learned what phrases work and which&nbsp;don&#8217;t.</li>
<li>Short phrases are much harder for the computer to properly transcribe. Brevity is punished. The more words you use to express less information, the better it works. “Paste” is almost never detected&nbsp;correctly.</li>
<li>Assuming you don&#8217;t have a custom transcription language model for your application, it will be very hard to get the computer to hear what you are saying if you are required to use odd phrases or terms. It&#8217;s like having a stenographer that just doesn&#8217;t know anything about your&nbsp;domain.</li>
<li>I do wonder if there’s something like <a href="https://en.wikipedia.org/wiki/Procedure_word">procedure words</a> for voice interfaces. You’ll know these from words like “roger” and “mayday”, where there’s clear and unambiguous words to communicate vital&nbsp;information.</li>
</ol>
</li>
<li>Voice has constraints, but it also has advantages, mostly from using language.<ol>
<li>The biggest is that you can talk about things you can&#8217;t see. A <span class="caps">GUI</span> has to show everything you can act on (sometimes with doorways to other&nbsp;things)</li>
<li>You can&#8217;t see search queries. And so search is always a top use of voice! In general search is used for things you can’t see but want to&nbsp;find.</li>
<li>Things-you-can&#8217;t-see is also important in multitasking. You can&#8217;t see the content of other tabs, and often can&#8217;t see other applications. I don&#8217;t think we&#8217;ve figured out how to unleash this, but I think there&#8217;s something&nbsp;there.</li>
<li>Because of the imprecision of voice everything is a search even if you wish it wasn&#8217;t. So something as simple as adding a bookmark to a folder involves searching for the&nbsp;folder.</li>
<li>You also can&#8217;t see things that don&#8217;t exist yet. I can imagine some utility in using language to create conditionals and triggers, using language’s ability to talk about something that does not yet exist. What excites me here isn’t that voice is necessarily easier, but that it’s easy to take phrases that do something now and rephrase them to talk about the future. Learning to talk about the future is implicit when learning to act right&nbsp;now.</li>
</ol>
</li>
<li>For all the cool things one might imagine doing with natural language interfaces, right now it&#8217;s all imperative commands.<ol>
<li>This is in part a human’s relationship with the computer. You don&#8217;t care what the computer thinks, you don&#8217;t need to theorize on what will happen or give it reassurance, you just tell it what to&nbsp;do.</li>
<li>Conveniently English imperatives are simpler than most other sentences. This is probably no coincidence, as an imperative is designed to be understood and acted on&nbsp;unambiguously.</li>
<li>But there may be something useful about using voice to create descriptions in parallel with other activities. For instance some researchers have found a benefit in having the user describe their actions while a recorder sees the concrete (but noisy) clicks and&nbsp;movements.</li>
<li>Maybe tagging and organization are useful voice tasks, to be done in parallel with other non-voice navigation. Voice could be a layer on top of normal&nbsp;interfaces.</li>
</ol>
</li>
<li>Access to microphones is hard and a big deal.<ol>
<li>There&#8217;s lots of very reasonable privacy&nbsp;concerns.</li>
<li>Like many privacy concerns, they are solved by making things harder for&nbsp;everyone.</li>
<li>Analog hardware is just difficult to handle, different hardware performs differently, things cut out or fail over&nbsp;time.</li>
<li>Custom hardware, like in the Alexa, makes really useful&nbsp;improvements.</li>
</ol>
</li>
<li>The big-bag-of-intent-handlers approach to parsing causes some problems<ol>
<li>As far as I can tell every system uses some form of categorization/classification, mapping a phrase to a handler. There’s always overlap, so you have to decide which handler is&nbsp;best.</li>
<li>Given variable inputs (search query, artist name, etc), mistranscriptions, and stopwords, the complete space of inputs is hard to&nbsp;enumerate.</li>
<li>As a result adding new handlers can have unexpected results, throwing the balance of the system off. I’ve been unimpressed with the extensibility of most assistants, but it’s&nbsp;understandable.</li>
<li>You can imagine always preferring native handlers to extension handlers, but that’s not great either. You want the handlers to be fairly broad, and it’s likely new handlers will be a refinement on functionality that’s merely a best-effort fallback for the default&nbsp;handlers.</li>
</ol>
</li>
<li>Apparently the vanguard of technology are now marketing agencies.<ol>
<li>Growth in these areas seems to be driven by someone saying &#8220;you know, voice is the&nbsp;future&#8230;&#8221;</li>
<li>The marketplaces for voice assistant skills are pretty awful for both consumers and producers. Consumers find junk, producers can&#8217;t find an&nbsp;audience.</li>
<li>Media outlets are the exception, but only because they are providing content instead of an&nbsp;interface.</li>
<li>Everything people do with an assistant seems stable and frozen. People do factual searches, weather, timers, reminders, and turn on and off&nbsp;lights.</li>
<li>The amount of technology we&#8217;re bringing to bear on a replacement for The Clapper is&nbsp;impressive.</li>
<li>Voice assistants, home and otherwise, are successful, but they are not successful&nbsp;ecosystems.</li>
</ol>
</li>
<li>It&#8217;s neat that people tell you exactly what they want to do.<ol>
<li>If you try to understand a <span class="caps">GUI</span> from behavioral telemetry you have to figure out why maybe someone hit a button and then canceled and tried a different button, and that maybe implies they wanted to do something that isn&#8217;t directly exposed, or maybe they just misclicked,&nbsp;or&#8230;</li>
<li>Because voice discovery is itself exploratory (if you want to find out if something works you should just try it), people will say what they&nbsp;want.</li>
<li>Of course there&#8217;s nothing that demands that an intent be clear enough to <em>ever</em> be implementable. So you may just be exposed to desires you can never&nbsp;fulfill.</li>
</ol>
</li>
<li>Voice doesn&#8217;t mean we can create smart agents that take care of everything.<ol>
<li>Voice makes smart agents seductive because you can express a desire using natural language without specifying exactly how to accomplish the&nbsp;thing.</li>
<li>You want a voice agent to buy tickets or order you dinner? No, you don&#8217;t want this. This isn&#8217;t going to be any more successful than asking a waiter to order for you. Maybe if you are adventurous or very familiar with the waiter it’s possible. Even in our family we&#8217;re constantly asking each other questions about preference, and negotiating options for small questions, and we are <em>very</em> familiar with each&nbsp;other.</li>
</ol>
</li>
<li>Being able to state intentions instead of specific actions offers some opportunity to support greater focus and directed action.<ol>
<li>The way you do something with a normal <span class="caps">GUI</span> is you come up with the goal in your mind, decompose it into actions, and then start on the first action. Maybe open a tab, click on a button, find a document, etc. It’s easy to get lost along the way – not just confused, but also&nbsp;distracted.</li>
<li>I doubt we can – or even should – just “make it happen” when there’s a complex goal-oriented statement. But even if the tool can’t easily break down a task, maybe the user can construct their own top-down outline of a&nbsp;task.</li>
<li>This immediately leads to the idea you could then save the outline as a repeatable task. Instead of making it an opaque repeatable task, I suspect it would be better to make it a list, and make it easy to follow along with lists. Then the assistant says “next you did: ‘open most recent email from Joe’; say “ok” or a command…” and then maybe that’s the right next task, or maybe you say something else inspired by your past command. The assistant can provide task scaffolding instead of&nbsp;automation.</li>
</ol>
</li>
<li>I am skeptical about learning and adaptation.<ol>
<li>Reliability – even reliably making mistakes – is an important feature. It means the user can learn about the system and adapt their behavior, without the system foiling them by changing its own&nbsp;behavior.</li>
<li>Discovery is hard, and having the search space change under your feet only makes it&nbsp;worse.</li>
<li>In summary: humans learn faster and better than machines. If the experience is going to grow, it needs to be explicit and deterministic, not clever or&nbsp;implied.</li>
</ol>
</li>
<li>Voice output suffers from too much or not enough information.<ol>
<li>This is where impolite interruptions might make voice output&nbsp;feasible.</li>
<li>Human voice interactions can have the same problem. We improve our communication by being in dialog with someone instead of just talking at them, and by using body language to interpret the interest of the other&nbsp;person.</li>
<li>Some of the equivalents – asking for confirmation before speaking more, or allowing the person to interrupt – might suffer from taking too many cues from human interaction. The questions themselves easily take more time and effort than just presenting too much information, and the interruptions turn the interaction into something that feels hostile instead of&nbsp;helpful.</li>
</ol>
</li>
<li>There’s “intent parsing” but not much I would consider “understanding”. There’s not much U in <a href="https://en.wikipedia.org/wiki/Natural-language_understanding"><span class="caps">NLU</span></a>.<ol>
<li>Intent parsing means: given an utterance, pick the most likely thing your program can do; and also: pull out some variable parameters from the&nbsp;utterance</li>
<li>There’s no room for understanding there. “Picking an intent handler” isn’t&nbsp;understanding.</li>
<li>You can only have understanding if you also have a knowledge model. Some flat <span class="caps">JSON</span> with some labels isn’t a model. Models in turn need to be attached to functional results: stuff the assistant can actually&nbsp;do.</li>
<li>I don’t know what understanding looks like, or what those models will look like. And I’m not even sure I’ll recognize them when I see them, it’s possible even in the code they’ll be&nbsp;implied.</li>
<li>As an aside, I personally believe action and meaning and understanding all go together: meaning only exists when we can attach it to effects, and abstract understanding is backfilled. So models without handlers aren’t meaningful, and models that are only handler references aren’t&nbsp;models.</li>
</ol>
</li>
<li>We could use more orthogonality. That is: meaningful phrase modifiers and statements that can apply across a wide variety of actions or application functionality.<ol>
<li>Orthogonality allows a user to come up with creative and unpredicted combinations in a phrase, and for those to actually&nbsp;work.</li>
<li>The simplest form of this I’ve thought about is a simple verb/direct-object with a simple set of&nbsp;modifiers.</li>
<li>Another way to think about it might be akin to <a href="https://www.ianbicking.org/blog/2018/02/web-small-composable-tools.html">composability</a>.</li>
</ol>
</li>
<li>Discovery is naturally hard.<ol>
<li>Natural language commands don’t need to be organized into hierarchical structures of menus or screens, which is nice but doesn’t lend itself to&nbsp;navigation.</li>
<li>Periodically showing the user some examples seems to be the state of the art.&nbsp;Sad.</li>
<li>I feel like something might be possible with cuing. I am reminded of a small anecdote from Logo, where when you type something that isn’t defined (e.g., <span class="caps">SQUARE</span>) it will reply <code>I don’t know how&nbsp; TO SQUARE</code>, hinting in the error message what you should do next (<code>TO PROC</code> defines a procedure). Doing this requires a cleverness in the language design that has not revealed itself to&nbsp;me.</li>
<li>Besides errors, any output (speech or text) is an opportunity to use language that suggests what phrases can be used. It’s a chance to encourage the user to mirror the computer’s&nbsp;language.</li>
<li>The big bag of intent handlers approach means there’s no natural comprehensibility to the space of phrases and abilities. By pursuing “natural language” we create a very sparse space of successful phrases amid the entirety of possible language. Is it&nbsp;predictable?</li>
</ol>
</li>
<li>Intents tend to be dominated by verbs, but I think we might do better leading with nouns.<ol>
<li>The parsers tend to focus on the constant bits, the verbs: “<strong>play</strong> [artist]”, “<strong>send message to</strong>&nbsp;[recipient]”.</li>
<li>This maps to a call like <code>sendMessage(“Emily”, “On my way home”)</code>. But maybe it should map to something like <code>search(“Emily”).sendMessage(“On my way home”)</code></li>
<li>The point being that you want the intersection of “entities called <em>Emily</em>” and &#8220;entities that respond to <em>sendMessage</em>”.</li>
</ol>
</li>
<li><span class="caps">STT</span> (speech to text) and <span class="caps">TTS</span> (text to speech) are terrible terms.<ol>
<li>I have to <em>think hard</em> to get the right one. Every&nbsp;time.</li>
<li>I prefer “speech/voice transcription” and “speech/voice&nbsp;generation”.</li>
</ol>
</li>
<li>Twenty seems like a good round number for a conclusion.<ol>
<li>It feels like there’s two paths before us: structured and unstructured understanding. Both ultimately lead to structured understanding, humans construct syntactic and meaningful structured statements. Is the bridge to that a structured pidgin, or an unstructured statistical&nbsp;understanding?</li>
<li>Fundamental user interface standards in voice are still mysteries to us. Will a radical change and consolidation happen like with <a href="https://en.wikipedia.org/wiki/WIMP_(computing)"><span class="caps">WIMP</span></a>?</li>
<li>A whole new level of expressivity <em>might</em> be revealed by voice interfaces. But I don’t think we know what that kind of relationship with a computer should look like. Current voice UIs are imperative just like graphical interfaces are&nbsp;imperative.</li>
<li>The details all matter: failures in speech recognition, different listening modes, microphone access and quality, output length and intonation, and all that ignores the actual <em>functionality</em> of the thing you are interfacing with, which also will require&nbsp;changes.</li>
<li>This clearly is going to happen, but I’m not at all certain the next shift will be centered on voice, or centered on something else and happen to include voice. As an analogy, touch interfaces enabled important changes… but touch interfaces themselves aren’t&nbsp;important.</li>
</ol>
</li>
</ol>
<p>Comments on <a href="https://twitter.com/ianbicking/status/1290310037198516227">Twitter</a> or <a href="https://news.ycombinator.com/item?id=24040539">Hacker News</a>.</p>
<hr>
<blockquote><p><a href="#footnote1-source" id="footnote1">**</a> From <i>The Fourth Intercometary</i> (a story I enjoyed and would recommend!): &#8220;Director Lester Bragolio stepped out of seclusion, clad in tunic, breeches and slippers, hairbrush in hand. He spoke while combing his tousled white hair. &#8216;Would you bring up the navscreen, please? Ten degree&nbsp;radius?&#8217;</p><p><span class="dquo">&#8220;</span>The Tipsy Witch held faithful to Yossi’s voice. He said &#8216;Navscreen A, grid out from Gledhill, ten degree radius.&#8217; Only then did the monitor come to life. &#8216;False color, please,&#8217; Yossi instructed. His own &#8216;please&#8217; meant &#8216;end of command.&#8217; Language changed when one talked to machines. Spoken as the Director said it, the word almost signified: you are the same to me as some piece of&nbsp;equipment.&#8221;</p></blockquote></div>
        <!-- /.entry-content -->
  
        <hr />

        <!-- <div>
            Hello! Did you know as of December 2024 I'm looking for a job? <a href="https://www.linkedin.com/feed/update/urn:li:activity:7265435901009231872/">I am!</a> I really like working with LLMs, especially in the domain of education, wellness, and <a href="https://en.wikipedia.org/wiki/Executive_functions">executive function</a>. Maybe <a href="mailto:ianbicking@gmail.com">drop me an email</a>?
        </div> -->
    </article>
</section>
        <section id="extras" class="body">
                <div class="links">
                  <h2><a href="https://ianbicking.org">here</a></h2>
                  <ul>
                    <li><a href="/blog/">blog</a></li>
                    <li><a href="/projects.html">projects</a></li>
                    <li><a href="https://ianbicking.org/archives.html">archives</a> &amp; <a href="https://ianbicking.org/categories.html">categories</a></li>
                    <li><a href="https://ianbicking.org/category/ai.html">category: ai</a></li>
                    <li><a href="https://ianbicking.org/category/javascript.html">category: javascript</a></li>
                    <li><a href="https://ianbicking.org/category/misc.html">category: misc</a></li>
                    <li><a href="https://ianbicking.org/category/mozilla.html">category: mozilla</a></li>
                    </ul>
                </div>
                <div class="social">
                        <h2>elsewhere</h2>
                        <ul>
                            <li><a href="https://ianbicking.org/feeds/atom.xml" type="application/atom+xml" rel="alternate">atom feed</a></li>

                            <li><a href="https://hachyderm.io/@ianbicking">@ianbicking@hachyderm.io</a></li>
                            <li><a href="https://bsky.app/profile/ianbicking.org">Blue Sky</a></li>
                            <li><a href="https://www.threads.net/@ibicking">Threads</a></li>
                            <li><a href="https://github.com/ianb">Github</a></li>
                            <li><a href="https://www.linkedin.com/in/ianbicking/">LinkedIn</a></li>
                        </ul>
                </div><!-- /.social -->
                <div class="archives">
                  <h2><a href="https://ianbicking.org/blog/">recent posts</a></h2>
                  <ul>
                    <li><a href="https://ianbicking.org/blog/2025/05/the-hungry-ghost.html">The Hungry&nbsp;Ghost</a></li>
                    <li><a href="https://ianbicking.org/blog/2024/05/ai-aita.html"><span class="caps">AI</span> <span class="caps">AITA</span></a></li>
                    <li><a href="https://ianbicking.org/blog/2024/04/roleplaying-by-llm.html">Roleplaying driven by an <span class="caps">LLM</span>: observations <span class="amp">&amp;</span> open&nbsp;questions</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/04/world-building-gpt-2-declarative.html">World Building with <span class="caps">GPT</span> part 2: bigger, better, more&nbsp;declarative</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/02/world-building-with-gpt.html">World Building With <span class="caps">GPT</span></a></li>
                    <li><a href="https://ianbicking.org/blog/2023/01/thoughts-on-voice-interfaces-2-llms.html">Thoughts On Voice Interfaces 2 years later:&nbsp;LLMs</a></li>
                    <li><a href="https://ianbicking.org/blog/2023/01/infinite-ai-array.html">Infinite <span class="caps">AI</span>&nbsp;Array</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/11/firefox-was-always-enough.html">Firefox Was Always&nbsp;Enough</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/09/project-ideas-2020.html">Project ideas for (what&#8217;s left of)&nbsp;2020</a></li>
                    <li><a href="https://ianbicking.org/blog/2020/09/a-history-of-projects.html">A History Of&nbsp;Projects</a></li>
                  </ul>
                </div><!-- /.archives -->

        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
          This is the personal site of <a href="/">Ian Bicking</a>.  The opinions expressed here are my own.
        </footer><!-- /#contentinfo -->

<script src="/theme/instantclick.min.js"></script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-FKT4HDGBE4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-FKT4HDGBE4');
</script>
        </div><!-- /#main-container -->
        </div><!-- /#main-wrapper2 -->
        </div><!-- /#main-wrapper1 -->
</body>
</html>